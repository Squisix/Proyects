{"cells":[{"cell_type":"markdown","metadata":{"id":"o3pNlhFcDksZ"},"source":["# Ejemplo de regularización con regresión logística\n","\n","## Reconocimiento de Patrones\n","\n","## Licenciatura en Ciencias de la Computación\n","### Universidad de Sonora\n","\n","**Ivo Jiménez** y **Julio Waissman**, 2022\n","\n","[Abrir en google Colab](https://github.com/ml-unison/ml-unison.github.io/raw/master/libretas/regularizacion_logistica.ipynb)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"lYzPjGG2Dkse","executionInfo":{"status":"ok","timestamp":1646198226688,"user_tz":420,"elapsed":3,"user":{"displayName":"Marco Vasquez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPqhF3TdfRdsUDpHDrhypJjotOoCkAIXBL9t63=s64","userId":"04561064872127784969"}}},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = (10,5)\n","plt.style.use('ggplot')"]},{"cell_type":"markdown","metadata":{"id":"Aqrro_S9Dksg"},"source":["## 1. La regresión logística ya programada\n","\n","Esto, antes de agregarle la regularización"]},{"cell_type":"markdown","metadata":{"id":"mUJC_dbqDksh"},"source":["La función logística está dada por \n","\n","$$\n","\\sigma(z) = \\frac{1}{1 + e^{-z}},\n","$$\n","\n","la cual es importante que podamos calcular en forma vectorial. Si bien el calculo es de una sola linea, el uso de estas funciones auxiliares facilitan la legibilidad del código.\n","\n","#### Desarrolla la función logística, la cual se calcule para todos los elementos de un ndarray."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"aIwFN-tfDksi","executionInfo":{"status":"ok","timestamp":1646198282350,"user_tz":420,"elapsed":573,"user":{"displayName":"Marco Vasquez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPqhF3TdfRdsUDpHDrhypJjotOoCkAIXBL9t63=s64","userId":"04561064872127784969"}},"outputId":"683ea035-4ff5-45b6-d882-1c2f1b7a81f3"},"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["def logistica(z):\n","    \"\"\"\n","    Calcula la función logística para cada elemento de z\n","    \n","    @param z: un ndarray\n","    @return: un ndarray de las mismas dimensiones que z\n","    \"\"\"\n","    return 1 / (1 + np.exp(-z))\n","\n","# Y ahora vamos a ver como se ve la función logística\n","z = np.linspace(-5, 5, 100)\n","plt.plot( z, logistica(z))\n","plt.title(u'Función logística', fontsize=20)\n","plt.xlabel(r'$z$', fontsize=20)\n","plt.ylabel(r'$\\frac{1}{1 + \\exp(-z)}$', fontsize=26)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Fjb68BY3Dksi"},"source":["Una vez establecida la función logística, vamos a implementar la función de error *sin regularizar* (error *en muestra*), la cual está dada por\n","\n","$$\n","E_{in}(w, b) = -\\frac{1}{M} \\sum_{i=1}^M \\left[ y^{(i)}\\log(a^{(i)}) + (1 - y^{(i)})\\log(1 - a^{(i)})\\right],\n","$$\n","\n","donde \n","\n","$$\n","a^{(i)} = \\sigma(z^{(i)}), \\quad\\quad z^{(i)} = w^T x^{(i)} + b\n","$$"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[],"id":"29GBjw3xDksj","executionInfo":{"status":"ok","timestamp":1646198285436,"user_tz":420,"elapsed":189,"user":{"displayName":"Marco Vasquez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPqhF3TdfRdsUDpHDrhypJjotOoCkAIXBL9t63=s64","userId":"04561064872127784969"}}},"outputs":[],"source":["def error_in(x, y, w, b):\n","    \"\"\"\n","    Calcula el error en muestra para la regresión logística\n","    \n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n","    @param w: un ndarray de dimensión (n, ) con los pesos\n","    @param b: un flotante con el sesgo\n","\n","    @return: un flotante con el valor de pérdida\n","    \n","    \"\"\" \n","    a = logistica(x @ w + b)\n","    return -np.mean(y * np.log(a) + (1 - y) * np.log(1 - a))\n","   \n","# El testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n","w = np.array([1])\n","b = 1.0\n","\n","x = np.array([[10],\n","              [-5]])\n","\n","y1 = np.array([1, 0])\n","y2 = np.array([0, 1])\n","y3 = np.array([0, 0])\n","y4 = np.array([1, 1])\n","\n","y_est = logistica(x @ w + b)\n","\n","assert abs(error_in(x, y1, w, b) - (-np.log(y_est[0]) - np.log(1 - y_est[1])) / 2) < 1e-2\n","assert abs(error_in(x, y2, w, b) - (-np.log(1 - y_est[0]) - np.log(y_est[1])) / 2) < 1e-2\n","assert abs(error_in(x, y3, w, b) - (-np.log(1 - y_est[0]) - np.log(1 - y_est[1])) / 2) < 1e-2\n","assert abs(error_in(x, y4, w, b) - (-np.log(y_est[0]) - np.log(y_est[1])) / 2) < 1e-2"]},{"cell_type":"markdown","metadata":{"id":"gCDAdHBCDksl"},"source":["De la misma manera, para poder implementar las funciones de aprendizaje, vamos a implementar el gradiente de la función de pérdida. El gradiente de la función de pérdida respecto a $\\omega$ $\\nabla E_in(w)$, y la derividada parciel respecto al sesgo se obtienen con las siguientes ecuaciones:\n","\n","$$\n","\\frac{\\partial E_{in}(w, b)}{\\partial w_j} = -\\frac{1}{M} \\sum_{i=1}^M \\left(y^{(i)} - a^{(i)}\\right)x_j^{(i)} \n","$$\n","\n","$$\n","\\frac{\\partial E_{in}(w, b)}{\\partial b} = -\\frac{1}{M} \\sum_{i=1}^M \\left(y^{(i)} - a^{(i)}\\right) \n","$$\n","\n","Todo esto **para el caso sin regularizar**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EwGc7fejDksl"},"outputs":[],"source":["def gradiente_error(x, y, w, b):\n","    \"\"\"\n","    Calcula el gradiente de la función de error en muestra.\n","    \n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n","    @param w: un ndarray de dimensión (n, ) con los pesos\n","    @param b: un flotante con el sesgo\n","    \n","    @return: dw, db, un ndarray de mismas dimensiones que w y un flotnte con el cálculo de \n","             la dervada evluada en el punto w y b\n","                 \n","    \"\"\"\n","    M = x.shape[0]\n","    error = y - logistica(x @ w + b)\n","    dw = -x.T @ error / M\n","    db = - error.mean()    \n","    return dw, db\n","\n","# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n","w = np.array([1])\n","b = 1.0\n","\n","x = np.array([[10],\n","              [-5]])\n","\n","y1 = np.array([1, 0])\n","y2 = np.array([0, 1])\n","y3 = np.array([0, 0])\n","y4 = np.array([1, 1])\n","\n","assert abs(0.00898475 - gradiente_error(x, y1, w, b)[1]) < 1e-4\n","assert abs(7.45495097 - gradiente_error(x, y2, w, b)[0]) < 1e-4 \n","assert abs(4.95495097 - gradiente_error(x, y3, w, b)[0]) < 1e-4 \n","assert abs(-0.49101525 - gradiente_error(x, y4, w, b)[1]) < 1e-4     "]},{"cell_type":"markdown","metadata":{"id":"stpkEUtyDksm"},"source":["## 2. Descenso de gradiente"]},{"cell_type":"markdown","metadata":{"id":"hJb_nIl3Dksn"},"source":["Ahora vamos a desarrollar las funciones necesarias para realizar el entrenamiento y encontrar la mejor $\\omega$ de acuero a la función de costos y un conjunto de datos de aprendizaje.\n","\n","Para ilustrar el problema, vamos a utilizar una base de datos sintética proveniente del curso de [Andrew Ng](www.andrewng.org/) que se encuentra en [Coursera](https://www.coursera.org). \n","\n","Supongamos que pertenecemos al departamente de servicios escolares de la UNISON y vamos a modificar el procedimiento de admisión. En lugar de utilizar un solo exámen (EXCOBA) y la información del cardex de la preparatoria, hemos decidido aplicar dos exámenes, uno sicométrico y otro de habilidades estudiantiles. Dichos exámenes se han aplicado el último año aunque no fueron utilizados como criterio. Así, tenemos un historial entre estudiantes aceptados y resultados de los dos exámenes. El objetivo es hacer un método de regresión que nos permita hacer la admisión a la UNISON tomando en cuenta únicamente los dos exámenes y simplificar el proceso. *Recuerda que esto no es verdad, es solo un ejercicio*.\n","\n","Bien, los datos se encuentran en el archivo `admision.txt` el cual se encuentra en formato `csv` (osea los valores de las columnas separados por comas. Vamos a leer los datos y graficar la información para entender un poco los datos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8ZNBmMoDksn"},"outputs":[],"source":["url = \"https://github.com/mcd-unison/aaa-curso/raw/main/ejemplos/admision.txt\"\n","datos = np.loadtxt(url, comments='%', delimiter=',')\n","\n","x, y = datos[:,0:-1], datos[:,-1] \n","\n","plt.plot(x[y == 1, 0], x[y == 1, 1], 'sr', label='aceptados') \n","plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazados')\n","plt.title(u'Ejemplo sintético para regresión logística')\n","plt.xlabel(u'Calificación del primer examen')\n","plt.ylabel(u'Calificación del segundo examen')\n","plt.axis([20, 100, 20, 100])\n","plt.legend(loc=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x-7et1QcDksn"},"source":["Vistos los datos un clasificador lineal podría ser una buena solución. Ahora vamos a implementar el método de descenso de gradiente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJKnjfBaDkso"},"outputs":[],"source":["def dg(x, y, w, b, alpha, max_iter=10_000, tol=1e-6, historial=False):\n","    \"\"\"\n","    Descenso de gradiente por lotes \n","\n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n","    @param alpha: Un flotante (típicamente pequeño) con la tasa de aprendizaje\n","    @param tol: Un flotante pequeño como criterio de paro. Por default 1e-6\n","    @param max_iter: Máximo numero de iteraciones. Por default 1e4\n","    @param historial: Un booleano para saber si guardamos el historial de errores\n","    \n","    @return: w, b, hist donde \n","        - w es ndarray de dimensión (n, ) con los pesos; \n","        - b es un float con el sesgo \n","        - hist, un ndarray de dimensión (max_iter,) \n","          con el valor del error en muestra en cada iteración. \n","          Si historial == False, entonces perdida_hist = None.\n","             \n","    \"\"\"\n","    M, n = x.shape    \n","    hist = [error_in(x, y, w, b)] if historial else None\n","    for epoch in range(1, max_iter):\n","        dw, db = gradiente_error(x, y, w, b)\n","        w -= alpha * dw\n","        b -= alpha * db\n","        error = error_in(x, y, w, b)\n","        if historial:\n","            hist.append(error)\n","        if np.abs(np.max(dw)) < tol:\n","            break \n","    return w, b, hist\n"]},{"cell_type":"markdown","metadata":{"id":"qa3sVZdsDkso"},"source":["Para probar la función de aprendizaje, vamos a aplicarla a nuestro problema de admisión. Primero recuerda que tienes que hacer una exploración para encontrar el mejor valor de $\\epsilon$. Así que utiliza el código de abajo para ajustar $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tweeogPDkso"},"outputs":[],"source":["alpha = 1e-4\n","mi = 50\n","w = np.zeros(x.shape[1])\n","b = 0.0\n","_, _, hist = dg(x, y, w, b, alpha, max_iter=mi, tol=1e-4, historial=True)\n","\n","plt.plot(np.arange(mi), hist)\n","plt.title(r'Evolucion del valor de la función de error en las primeras iteraciones con $\\alpha$ = ' + str(alpha))\n","plt.xlabel('iteraciones')\n","plt.ylabel('perdida')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BPUX90OoDksp"},"source":["Una vez encontrado el mejor $\\epsilon$, entonces podemos calcular $\\omega$ (esto va a tardar bastante), recuerda que el costo final debe de ser lo más cercano a 0 posible, así que agrega cuantas iteraciones sean necesarias (a partir de una función de pérdida con un valor de al rededor de 0.22 ya está bien). Puedes ejecutar la celda cuandas veces sea necesario con un número limitado de iteraciones (digamos unas 10,000) para ver como evoluciona. Esto podría mejorar sensiblementa si se normalizan los datos de entrada. Tambien puedes variar $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"PtSMFCzTDksp"},"outputs":[],"source":["w, b, _ = dg(x, y, w, b, 20*alpha, max_iter = 10_000)\n","print(\"Los pesos obtenidos son: \\n{}\".format(w))\n","print(\"El sesgo obtenidos es: \\n{}\".format(b))\n","print(\"El valor final de la función de pérdida es: {}\".format(error_in(x, y, w, b))) "]},{"cell_type":"markdown","metadata":{"id":"DnKe9PzfDksp"},"source":["Es interesante ver como el descenso de gradiente no es muy eficiente en este tipo de problemas, a pesar de ser problemas de optimización convexos.\n","\n","Bueno, este método nos devuelve $\\omega$, pero esto no es suficiente para decir que tenemos un clasificador, ya que un método de clasificación se compone de dos métodos, uno para **aprender** y otro para **predecir**. \n","\n","Recuerda que $a^{(i)} = \\Pr[y^{(i)} = 1 | x^{(i)} ; w, b]$, y a partir de esta probabilidad debemos tomar una desición. Igualmente recuerda que para tomar la desicion no necesitamos calcular el valor de la logística, si conocemos el umbral."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6YDIjPDDksp"},"outputs":[],"source":["def predictor(x, w, b):\n","    \"\"\"\n","    Predice los valores de y_hat (que solo pueden ser 0 o 1), utilizando el criterio MAP.\n","    \n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param w: un ndarray de dimensión (n, ) con los pesos\n","    @param b: un flotante con el sesgo\n","\n","    @return: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0 con la salida estimada\n","    \"\"\"\n","    return np.where(x @ w + b > 0, 1, 0)"]},{"cell_type":"markdown","metadata":{"id":"5RgEJJzsDksq"},"source":["¿Que tan bueno es este clasificador? ¿Es que implementamos bien el método?\n","\n","Vamos a contestar esto por partes. Primero, vamos a graficar los mismos datos pero vamos a agregar la superficie de separación, la cual en este caso sabemos que es una linea recta. Como sabemos el criterio para decidir si un punto pertenece a la clase distinguida o no es si el valor de $w^T x^{(i)} + b \\ge 0$, por lo que la frontera entre la región donde se escoge una clase de otra se encuentra en:\n","\n","$$\n","0 = b + w_1 x_1  + w_2 x_2,\n","$$\n","\n","y despejando:\n","\n","$$\n","x_2 = -\\frac{b}{w_2} -\\frac{w_1}{w_2}x_1\n","$$\n","\n","son los pares $(x_1, x_2)$ los valores en la forntera. Al ser estos (en este caso) una linea recta solo necesitamos dos para graficar la superficie de separación. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_8oryVEPDksq"},"outputs":[],"source":["x1_frontera = np.array([20, 100]) #Los valores mínimo y máximo que tenemos en la gráfica de puntos\n","x2_frontera = -(b / w[1]) - (w[0] / w[1]) * x1_frontera\n","\n","print(x1_frontera)\n","print(x2_frontera)\n","\n","plt.plot(x[y == 1, 0], x[y == 1, 1], 'sr', label='aceptados') \n","plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazados')\n","plt.plot(x1_frontera, x2_frontera, 'm')\n","plt.title(u'Ejemplo sintético para regresión logística')\n","plt.xlabel(u'Calificación del primer examen')\n","plt.ylabel(u'Calificación del segundo examen')\n","plt.axis([20, 100, 20, 100])\n","plt.legend(loc=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vFmE98snDksq"},"source":["## 3. Clasificación polinomial\n","\n","Como podemos ver en la gráfica de arriba, parece ser que la regresión logística aceptaría a algunos estudiantes rechazados y rechazaría a algunos que si fueron en realidad aceptados. En todo método de clasificación hay un grado de error, y eso es parte del poder de generalización de los métodos. \n","\n","Sin embargo, una simple inspección visual muestra que, posiblemente, la regresión lineal no es la mejor solución, ya que la frontera entre las dos clases parece ser más bien una curva.\n","\n","¿Que tal si probamos con un clasificador cuadrático? Un clasificador cuadrático no es más que la regresión lineal pero a la que se le agregan todos los atributos que sean una combinación de dos de los atributos. \n","\n","Por ejemplo, si un ejemplo $x = (x_1, x_2, x_3)^T$ se aumenta con todas sus componentes cuadráticas, entonces tenemos los atributos\n","\n","$$\n","\\phi_2(x) = (x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3, x_1^2, x_2^2, x_3^2)^T.\n","$$ \n","\n","De la misma manera se pueden obtener clasificadores de orden tres, cuatro, cinco, etc. En general a estos clasificadores se les conoce como **clasificadores polinomiales**. Ahora, para entender bien la idea, vamos a resolver el problema anterior con un clasificador de orden 2. \n","\n","Sin embargo, si luego se quiere hacer el reconocimiento de otros objetos, o cambiar el orden del polinomio, pues se requeriría de reclcular cada vez la expansión polinomial. Vamos a generalizar la obtención de atributos polinomiales con la función `map_poly`, la cual la vamos a desarrollar a continuación.\n","\n","En este caso, la normalización de los datos es muy importante, por lo que se agregan las funciones pertinentes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoWrmti-Dksr"},"outputs":[],"source":["from itertools import combinations_with_replacement\n","\n","def map_poly(grad, x):\n","    \"\"\"\n","    Encuentra las características polinomiales hasta el grado grad de la matriz de datos x, \n","    asumiendo que x[:n, 0] es la expansión de orden 1 (los valores de cada atributo)\n","    \n","    @param grad: un entero positivo con el grado de expansión\n","    @param x: un ndarray de dimension (M, n) donde n es el número de atributos\n","    \n","    @return: un ndarray de dimensión (M, n_phi) donde\n","             n_phi = \\sum_{i = 1}^grad fact(i + n - 1)/(fact(i) * fact(n - 1))\n","    \"\"\"\n","    \n","    if int(grad) < 2:\n","        raise ValueError('grad debe de ser mayor a 1')\n","    \n","    M, n = x.shape\n","    atrib = x.copy()\n","    x_phi = x.copy()\n","    for i in range(2, int(grad) + 1):\n","        for comb in combinations_with_replacement(range(n), i):\n","            x_phi = np.c_[x_phi, np.prod(atrib[:, comb], axis=1)]\n","    return x_phi   \n","\n","def medias_std(x):\n","    \"\"\"\n","    Obtiene un vector de medias y desviaciones estandar para normalizar\n","    \n","    @param x: Un ndarray de (M, n) con una matriz de diseño\n","    \n","    @return: mu, des_std dos ndarray de dimensiones (n, ) con las medias y desviaciones estandar\n","    \n","    \"\"\"\n","    return np.mean(x, axis=0), np.std(x, axis=0)\n","\n","def normaliza(x, mu, des_std):\n","    \"\"\"\n","    Normaliza los datos x\n","    \n","    @param x: un ndarray de dimension (M, n) con la matriz de diseño\n","    @param mu: un ndarray (n, ) con las medias\n","    @param des_std: un ndarray (n, ) con las desviaciones estandard\n","    \n","    @return: un ndarray (M, n) con x normalizado\n","    \n","    \"\"\"\n","    return (x - mu) / des_std\n"]},{"cell_type":"markdown","metadata":{"id":"7CiUoGEjDksr"},"source":["**Realiza la clasificación de los datos utilizando un clasificador cuadrático (recuerda ajustar primero el valor de $\\alpha$)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERKCAVeTDkss"},"outputs":[],"source":["# Encuentra phi_x (x son la expansión polinomial de segundo orden, utiliza la función map_poly\n","phi_x =  map_poly(2, x) \n","mu, de = medias_std(phi_x)\n","phi_x_norm = normaliza(phi_x, mu, de)\n","\n","# Utiliza la regresión logística\n","alpha = None #--Agrega el valor aqui--\n","w = np.zeros(phi_x.shape[1]) \n","b = 0 \n","_, _, hist = dg(phi_x_norm, y, w, b, alpha, max_iter=10_000, historial=True)\n","\n","plt.plot(range(len(hist)), hist)\n","plt.xlabel('epochs')\n","plt.ylabel(r'$E_{in}$')\n","plt.title('Evaluación del parámetro alpha')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Zi2H5ZiwDkss"},"outputs":[],"source":["alpha = 1\n","w_norm, b_norm, _ = dg(phi_x_norm, y, w, b, alpha, 100_000)\n","\n","print(\"Los pesos obtenidos son: \\n{}\".format(w_norm))\n","print(\"El sesgo obtenidos es: \\n{}\".format(b_norm))\n","print(\"El error en muestra es: {}\".format(error_in(phi_x_norm, y, w_norm, b_norm))) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"x-dXqleqDkss"},"source":["donde se puede encontrar un error en muestra de aproximadamente 0.03."]},{"cell_type":"markdown","metadata":{"id":"SlfwzXQKDkss"},"source":["Si revisas, usando las ecuaciones como tal, pues hace que tengamos una división por 0, dado que tenemos una operación $\\log(a)$ para alguna $a$ que tiende a 0 (o podría ser para algun $\\log(1-a)$ para alguna $a$ que tiene a 1). \n","\n","Para evitar esto, hay que regresarnos y cambiar la operación del cálculo del error en linea por algo como esto:\n","\n","```python\n","def error_in(x, y, w, b):\n","    \"\"\"\n","    Calcula el error en muestra para la regresión logística\n","    \n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n","    @param w: un ndarray de dimensión (n, ) con los pesos\n","    @param b: un flotante con el sesgo\n","\n","    @return: un flotante con el valor de pérdida\n","    \n","    \"\"\" \n","    a = logistica(x @ w + b)\n","    return -np.nansum(\n","            np.sum(np.log(a[y > 0.5])) + \n","            np.sum(np.log(1 - a[y < 0.5]))\n","        ) / a.shape[0]\n","```"]},{"cell_type":"markdown","metadata":{"id":"2_JyZpK4Dkst"},"source":["\n","Vamos a graficar el resultado. Para graficar la separación de datos en una proyección en las primeras dos dimensiones vamos a tener que generar un `contour`, y sobre este graficar los datos. Para esto vamos a desarrollar una función."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjM7oLhADkst"},"outputs":[],"source":["def plot_separacion2D(x, y, grado, mu, de, w, b):\n","    \"\"\"\n","    Grafica las primeras dos dimensiones (posiciones 1 y 2) de datos en dos dimensiones \n","    extendidos con un clasificador polinomial así como la separación dada por theta_phi\n","    \n","    \"\"\"\n","    if grado < 2:\n","        raise ValueError('Esta funcion es para graficar separaciones con polinomios mayores a 1')\n","    \n","    x1_min, x1_max = np.min(x[:,0]), np.max(x[:,0])\n","    x2_min, x2_max = np.min(x[:,1]), np.max(x[:,1])\n","    delta1, delta2 = (x1_max - x1_min) * 0.1, (x2_max - x2_min) * 0.1\n","\n","    spanX1 = np.linspace(x1_min - delta1, x1_max + delta1, 600)\n","    spanX2 = np.linspace(x2_min - delta2, x2_max + delta2, 600)\n","    X1, X2 = np.meshgrid(spanX1, spanX2)\n","    X = normaliza(map_poly(grado, np.c_[X1.ravel(), X2.ravel()]), mu, de)\n","\n","    print(X1.shape, X2.shape, X.shape)\n","\n","    Z = predictor(X, w, b)\n","    Z = Z.reshape(X1.shape[0], X1.shape[1])\n","    \n","    print( Z[Z  < 0.5].shape)\n","\n","    #plt.contour(spanX1, spanX1, Z, linewidths=0.2, colors='k')\n","    plt.contourf(spanX1, spanX1, Z, 1, cmap=plt.cm.binary_r)\n","    plt.plot(x[y > 0.5, 0], x[y > 0.5, 1], 'sr', label='clase positiva')\n","    plt.plot(x[y < 0.5, 0], x[y < 0.5, 1], 'oy', label='clase negativa')\n","    plt.axis([spanX1[0], spanX1[-1], spanX2[0], spanX2[-1]])"]},{"cell_type":"markdown","metadata":{"id":"hQUzCgOrDkst"},"source":["Y ahora vamos a probar la función `plot_separacion2D` con los datos de entrenamiento. El comando tarda, ya que estamos haciendo un grid de 200 $\\times$ 200, y realizando evaluaciones individuales."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ywjCbJLDkst"},"outputs":[],"source":["plot_separacion2D(x, y, 2, mu, de, w_norm, b_norm)\n","plt.title(u\"Separación con un clasificador cuadrático\")\n","plt.xlabel(u\"Calificación del primer examen\")\n","plt.ylabel(u\"Calificación del segundo examen\")"]},{"cell_type":"markdown","metadata":{"id":"iFLMwIssDkst"},"source":["Como podemos ver, un clasificador polinomial de orden 2 clasifica mejor los datos de aprendizaje, y además parece suficientemente simple para ser la mejor opción para hacer la predicción. Claro, esto lo sabemos porque pudimos visualizar los datos, y en el fondo estamos haciendo trampa, al seleccionar la expansión polinomial a partir de una inspección visual de los datos. Vamos ahora a ver un problema más dificil de ver a simple vista para analizar que hace la regularización.\n","\n","## 4. Regularización\n","\n","Tomemos ahora una base de datos que si bien es sintética es representativa de una familia de problemas a resolver. Supongamos que estámos opimizando la fase de pruebas dentro de la linea de producción de la empresa Microprocesadores del Noroeste S.A. de C.V.. La idea es reducir el banco de pruebas de cada nuevo microprocesador fabricado y en lugar de hacer 50 pruebas, reducirlas a 2. En el conjunto de datos tenemos los valores que obtuvo cada componente en las dos pruebas seleccionadas, y la decisión que se tomo con cada dispositivo (esta desición se tomo con el banco de 50 reglas). Los datos los podemos visualizar a continuación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qY7FhaFNDkst"},"outputs":[],"source":["url = \"https://github.com/mcd-unison/aaa-curso/raw/main/ejemplos/prod_test.csv\"\n","datos = np.loadtxt(url, comments='%', delimiter=',')\n","\n","x, y = datos[:,0:-1], datos[:,-1] \n","\n","plt.plot(x[y == 1, 0], x[y == 1, 1], 'or', label='cumple calidad') \n","plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazado')\n","plt.title(u'Ejemplo de pruebas de un producto')\n","plt.xlabel(u'Valor obtenido en prueba 1')\n","plt.ylabel(u'Valor obtenido en prueba 2')\n","plt.legend(loc=0)"]},{"cell_type":"markdown","metadata":{"id":"j38a34nVDkst"},"source":["Cláramente este problema no se puede solucionar con un clasificador lineal (1 orden), por lo que hay que probar otros tipos de clasificadores.\n","\n","**Completa el código para hacer regresión polinomial para polinomios de orden 2, 4, 6 y 8, y muestra los resultados en una figura. Recuerda que este ejercicio puede tomar bastante tiempo de cómputo. Posiblemente tengas que hacer ajustes en el código para manejar diferentes valores de alpha y max_iter de acuerdo a cada caso**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rB5nz6BDDksu"},"outputs":[],"source":["for (i, grado) in enumerate([2, 6, 10, 14]):\n","\n","    # Genera la expansión polinomial\n","    # --- Agregar código aquí ---\n","    \n","    \n","    # Normaliza\n","    # --- Agregar código aquí ---\n","\n","    \n","    # Entrena\n","    # --- Agregar código aquí ---\n"," \n","\n","\n","    # Muestra resultados con plot_separacion2D \n","    plt.subplot(2, 2, i + 1)\n","    plt.title(f\"Polinomio de grado {grado}\")\n","    # --- Agregar codigo aquí ---\n","    # plot_separacion2D(...) Esto es solo para ayudarlos un poco\n","  "]},{"cell_type":"markdown","metadata":{"id":"BsWzrtz_Dksu"},"source":["Como podemos ver del ejercicio anterior, es dificil determinar el grado del polinomio, y en algunos casos es demasiado general (subaprendizaje) y en otros demasiado específico (sobreaprendizaje). \n","\n","¿Que podría ser la solución?, Una solución posible es utilizar un polinomio de alto grado (o relativamente alto), y utilizar la **regularización** para controlar la generalización del algoritmo, a través de una variable $\\lambda$.\n","\n","Recordemos, la función de costos de la regresión logística con regularización es:\n","\n","$$\n","costo(w, b) = E_{in}(w, b) + \\frac{\\lambda}{M} regu(w),\n","$$\n","\n","donde $regu(w)$ es una función de regularización, la cual puede ser $l_1$, $l_2$ u otras, tal como vimos en clase. \n","\n","**Completa el siguiente código, utilizando una regularización en $L_2$**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BD5h0txDksu"},"outputs":[],"source":["def costo(x, y, w, b, lambd):\n","    \"\"\"\n","    Calcula el costo de una w dada para el conjunto dee entrenamiento dado por y y x,\n","    usando regularización\n","    \n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n","    @param w: un ndarray de dimensión (n, ) con los pesos\n","    @param b: un flotante con el sesgo\n","    @param lambd: un flotante con el valor de lambda en la regularizacion\n","\n","    @return: un flotante con el valor de pérdida\n","    \n","    \"\"\" \n","    costo = 0\n","    M = x.shape[0]\n","    \n","    #------------------------------------------------------------------------\n","    # Agregua aqui tu código\n","    \n","    \n","    #------------------------------------------------------------------------\n","    return costo\n","\n","\n","def grad_regu(x, y, w, b, lambd):\n","    \"\"\"\n","    Calcula el gradiente de la función de costo regularizado para clasificación binaria, \n","    utilizando una neurona logística, para w y b y conociendo un conjunto de aprendizaje.\n","    \n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n","    @param w: un ndarray de dimensión (n, ) con los pesos\n","    @param b: un flotante con el sesgo\n","    @param lambd: un flotante con el peso de la regularización\n","    \n","    @return: dw, db, un ndarray de mismas dimensiones que w y un flotnte con el cálculo de \n","             la dervada evluada en el punto w y b\n","                 \n","    \"\"\"\n","    M = x.shape[0]\n","    dw = np.zeros_like(w)\n","    db = 0.0\n","\n","    #------------------------------------------------------------------------\n","    # Agregua aqui tu código\n","\n","    \n","    #------------------------------------------------------------------------\n","    return dw, db\n","\n","\n","def dg_regu(x, y, w, b, alpha, lambd, max_iter=10_000, tol=1e-4, historial=False):\n","    \"\"\"\n","    Descenso de gradiente con regularización l2\n","\n","    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n","    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n","    @param alpha: Un flotante (típicamente pequeño) con la tasa de aprendizaje\n","    @param lambd: Un flotante con el valor de la regularización\n","    @param max_iter: Máximo numero de iteraciones. Por default 10_000\n","    @param tol: Un flotante pequeño como criterio de paro. Por default 1e-4\n","    @param historial: Un booleano para saber si guardamos el historial\n","    \n","    @return: \n","        - w: ndarray de dimensión (n, ) con los pesos; \n","        - b:  float con el sesgo \n","        - hist: ndarray de (max_iter,) el historial de error. \n","                Si historial == False, entonces hist = None.\n","             \n","    \"\"\"\n","    M, n = x.shape    \n","    hist = [costo(x, y, w, b, lambd)] if historial else None\n","    for epoch in range(1, max_iter):\n","        dw, db = grad_regu(x, y, w, b, lambd)\n","        w -= alpha * dw\n","        b -= alpha * db\n","        error = costo(x, y, w, b, lambd)\n","        if historial:\n","            hist.append(error)\n","        if np.abs(np.max(dw)) < tol:\n","            break \n","    return w, b, hist\n"]},{"cell_type":"markdown","metadata":{"id":"1r4vk4dXDksv"},"source":["\n","**Desarrolla las funciones y scripts necesarios para realizar la regresión logística con un polinomio de grado `grado` y con cuatro valores de regularización diferentes. Grafica la superficie de separación para cuatro valores diferentes de $\\lambda$.**\n","\n","Realiza varias pruebas para encontrar cual es la relación de `grado` y de $\\lambda$ que sea un buen compromiso entre generalizar demasiado o tener sobreaprendizaje. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrWFnFrwDksv"},"outputs":[],"source":["grado = 5\n","phi_x = map_poly(grado, x)\n","mu, de = medias_std(phi_x)\n","phi_x_norm = normaliza(phi_x, mu, de)\n","\n","\n","for (i, lambd) in enumerate([0, 1, 10, 100]):\n","\n","    # Normaliza\n","    # --- Agregar código aquí ---\n","    \n","    # Entrena\n","    # --- Agregar código aquí ---\n","    \n","    # Muestra resultados con plot_separacion2D \n","    plt.subplot(2, 2, i + 1)\n","    plt.title(f\"Polinomio de grado {grado}, regu = {lambd}.\")\n","    # --- Agregar codigo aquí ---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iRhdb6MNDksv"},"source":["Es muy importante el análisis y las conclusiones que puedas sacar de este ejercicio.\n","\n","**Escribe aquí tus conclusiones**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"regularizacion_logistica.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}